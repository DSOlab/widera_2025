# Staff Exchange

## Staff Exchange Schemes

- Residency Sprint (2–8 weeks) : A short, output-driven placement embedded in a host 
  team to solve a focused problem, ship a feature, or harden a release. Fast onboarding, 
  tight scope, and a clear artifact on exit (code, SOP, dataset). Target staff & 
  direction: W→E (primary) and W↔W. Modality: one-to-one or one-to-few.

- Secondment (3–6 months): A deeper integration for roles that transfer processes 
  not just know-how. The secondee operates within the host’s routines and brings 
  back templates and workflows for institutional rollout. Target staff & 
  direction: W→E for process transfer roles W→W for institutionalisation. 
  Modality: one-to-many (embedded in host unit) or few-to-many for twin roles.

- Job Shadowing (1–2 weeks): Observe end-to-end operations in situ (e.g for finance 
  staff). The goal is to capture real forms, timelines, and decision points you 
  can mirror at home. Target staff & direction: W→E (RMA, legal/IPR, finance, ethics, 
  HR/OTM-R); W→W for benchmarking. Best-fit scope: administrative/management 
  workflows (proposal→grant→report→audit). Modality: one-to-one (mentor–observer) 
  with short rotations across desks.

- Reverse Exchange (Mentor to Widening Site, 1–2 weeks on site + virtual 
  follow-up): Instead of sending staff out, bring a mentor in to help run a pilot 
  on our infrastructure and tune local practices. Ideal for Living-Lab sprints and 
  on-prem deployments. Target staff & direction: E→W (primary); optionally W→W with 
  a trained champion as mentor. Best-fit scope: on-prem deployment & pilot (Living-Lab 
  sprint, dashboard go-live, local SOP adoption). Modality: one-to-many (mentor 
  with local team).

- Cohort Academy (1–2 weeks, once or twice/year): Mixed cohorts (researchers, 
  engineers, RMA) train together on a flagship topic, earn micro-credentials, 
  and co-author reusable teaching packs. Builds cross-partner networks and 
  common language. Target staff & direction: Mixed W+E participants, prioritise 
  W staff. Best-fit scope: foundational & cross-cutting skills (POD/Altimetry/Gravity/GNSS
   ops, FAIR, OSS, CoARA/OTM-R); ends with micro-credentials. Modality: many-to-many 
  (lecture + labs + group artifacts). In essence, the CoE's/Alliance’s twice-yearly, 
  one-week workshop+bootcamp, hosted in rotation by partners. It’s where we can 
  train at scale, certify skills, and co-create reusable assets.

- Virtual Co-Working Cycle (6–8 weeks, 1–2 hrs/week): Keeps momentum between 
  visits: weekly clinics, pair-programming, KPI reviews, and blockers triage, 
  all aligned to a shared sprint goal. Lightweight, inclusive, and easy to 
  scale. Target staff & direction: Mixed pods (W+E) sustaining sprint goals 
  between visits. Best-fit scope: harmonisation & follow-through. Modality: 
  many-to-many (stand-ups/clinics), with occasional one-to-one pairings.

## Schemas Based on Outcome/Objective

* Deep process change: Secondment (W→E).
* Feature/bug/ops hardening: Residency Sprint (W→E or W↔W).
* Policy/admin replication: Job Shadowing (W→E).
* Local adoption fast-track: Reverse Exchange (E→W).
* Baseline skills at scale: Cohort Academy (many-to-many).
* Sustain momentum: Virtual Co-Working (many-to-many + pairs).

## Verification and Link to CoARA

*just a thought ... *

Apart from KPIs to measure staff exchange effectiveness, we could also implement an
"Alliance Skills Registry" which will record verifiable recognition of CoE members 
involved in exchanges (**??is this a good idea??**). 

Participants in the staff-exchange schemes will earn verifiable recognition, not 
just attendance certificates. Depending on the track and the assessed output, this 
may be a **micro-credential**, a **graded artifact credential** (e.g., a merged code PR, 
a DOI’d dataset/benchmark, an SOP/SLA put into operation, a QA dashboard), or 
a **mentor/instructor badge** (for reverse exchanges or Train-the-Trainer). Each 
award carries embedded metadata (issuer, skills, workload, assessment method/date, 
evidence link) and is issued only after a rubric-based review (Pass/Merit/Distinction). 
Evidence lives in an Alliance Skills Registry and is linkable (DOI/PR/dashboard) 
to ensure verification and reuse.

These recognitions plug directly into the CoARA-aligned assessment framework. The 
Narrative CV includes a dedicated section for software, datasets, operational services 
and training, where participants cite their micro-credentials and artifact links; 
panels then score them using our published rubric. In short, exchanges generate 
traceable, evidence-backed achievements that are counted and valued; exactly the cultural 
shift CoARA expects.
