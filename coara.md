# CoARA-aligned Research Assessment Framework

We will co-develop a CoARA-aligned Research Assessment Framework that fits our alliance’s mission: equip widening universities with operational, shared capacity in space geodesy while reforming how excellence is recognised. The framework will shift evaluation away from journal proxies toward evidence of contribution and use, with a strong focus on open, FAIR, reproducible outputs. In geodesy terms, assessable contributions will explicitly include: released software/pipelines (POD, coastal altimetry, gravity, GNSS deformation), FAIR datasets/benchmarks with DOIs, operational services (SOPs, SLAs, QA dashboards), standards participation (IAG/GGOS, IGS/ILRS/IDS/EUREF), cal/val engagements, interdisciplinary uptake (e.g., Living Labs), and training/mentoring (micro-credentials, train-the-trainer). Narrative CVs will surface team science, technical leadership, data stewardship, reproducibility, open-source governance, and societal impact; quantitative indicators (uptime, latency, accuracy, reuse, adoption) complement qualitative peer review. The framework embeds OTM-R, bias mitigation, and diversity & inclusion safeguards.

Implementation is Alliance/CoE-wide: drafted by the **?? What-Board ??** with support from **??SomeOtherBoard??**, peer-reviewed by the External Advisory Board, and piloted internally before alliance/CoE adoption. We will provide a turn-key adoption kit—policy text, narrative-CV templates, panel handbooks, scoring rubrics, COI/appeals procedures, and training for evaluators—so each widening HEI can tailor and adopt the framework institutionally later on. Annual monitoring will track uptake (e.g., % evaluations using narrative CV; % decisions citing software/data/service evidence; gender/diversity balance; alignment with open-science practices), with published improvement cycles. In short, the framework rewards the real work that makes geodesy impactful—operational, open, and service-ready—while meeting the call’s objectives on institutional reform, digital capacity, skills, and uptake.

## Criteria/Metrics Overview

menu of metrics/criteria the CoARA-aligned framework will actually use (mixing quantitative + qualitative, all evidence-based):

* Research software & pipelines
    - Quant: releases with DOIs/versions; CI/CD pass rate; test coverage; vuln scans; issue MTTR.
    - Qual: maintainer role, architecture/docs quality, reusability and portability.

* FAIR datasets & benchmarks
    - Quant: DOIs; metadata completeness (CF/STAC/etc.); reuse/downloads.
    - Qual: clarity of provenance, suitability as a community reference.

* Operational service performance (SOPs/SLAs)
    - Quant: uptime %, p95 latency, SLA adherence, incident rate, MTTR.
    - Qual: robustness of SOPs, incident post-mortems, service improvement loop.

* Scientific validity & reproducibility
    - Quant: accuracy vs references (e.g., SLR residual RMS, overlap errors); number of fully reproducible runs.
    - Qual: transparency of methods, uncertainty treatment, limitations clearly stated.

* Adoption & reuse (impact-in-use)
    - Quant: # unique organisations integrated via API; citations of DOIs; MAUs; external contributions/forks/clones.
    - Qual: evidence of policy/operational uptake (MoUs, SLAs, inclusion in agency SOPs).

* Standards & community service
    - Quant: roles held (WG chair/member), contributions to conventions/specs, number of standardisation submissions, product inclusion to international services.
    - Qual: leadership influence, involvement in services (IGS/ILRS/IDS/EUREF/GGOS).

* Cal/Val & reference contributions
    - Quant: validation reports produced/accepted; # reference-site campaigns; cross-comparison rounds completed.
    - Qual: independence/rigour of cal/val, clarity of acceptance thresholds.

* Training, mentoring & skills building
    - Quant: trainees certified (micro-credentials), TtT multipliers, course iterations, satisfaction scores.
    - Qual: depth of mentoring, curriculum quality, alignment to institutional/CoE needs.

* Team science & leadership
    - Quant: cross-partner outputs; co-led workstreams; code/data co-ownership.
    - Qual: clarity of role in Narrative CV, coordination ability, inclusive leadership.

* Open science & licensing
    - Quant: % outputs open within target timelines; FAIR license compliance
    - Qual: appropriateness of licenses, clarity of reuse terms

* Interdisciplinary & ecosystem engagement (Living Labs)
    - Quant: pilot sprints completed; stakeholder NPS/feedback; # policy briefs.
    - Qual: relevance to user problems, quality of handover (SOPs, dashboards).

* Responsible research, ethics & security
    - Quant: audits passed (GDPR/security); # remedial actions closed on time.
    - Qual: risk management quality, data minimisation, ethical reflexivity.

* Diversity, inclusion & OTM-R practice
    - Quant: panel training completion; diversity of panels/applicant pools; EURAXESS postings.
    - Qual: evidence of bias-mitigation in decisions; inclusive supervision practices.

* Sustainability & maintenance
    - Quant: release cadence; bus-factor (≥2 maintainers); external support channels active.
    - Qual: realism of maintenance plan and funding pathway beyond the project.

Each criterion comes with evidence rules (links/DOIs/dashboards) and scoring bands; quantitative targets are set per flagship maturity (Y2/Y4) and complement qualitative peer review.

## Implementation Dashboard

- **Define scope & objectives** Action: List processes to be covered (hiring/promotion, seed calls, fellowships, awards) and success criteria. Outcome: scope note with targets and exclusions.

- **Baseline audit & gap analysis** Action: Collect current policies, forms, panel practices; identify gaps vs CoARA/OTM-R. Outcome: Gap matrix with required changes and priorities.

- **Set qualitative principles & criteria** Action: Write CoARA-aligned criteria (recognise software, datasets/DOIs, services/SOPs/SLAs, standards, training, impact).  Outcome: Principles document for endorsement.

- **Fix quantitative indicators** Action: Define metrics, formulas, and data sources (e.g., uptime, p95 latency, accuracy vs refs, reuse/adoption, time-to-decision, panel diversity). Outcome: Metrics catalogue with calculation rules and thresholds.

- **Evidence rules & verification** Action: Specify what counts as proof (DOIs, dashboards, logs), how links are checked, and who verifies.Outcome: Evidence checklist and verification SOP.

- **Issue Narrative-CV template** Action: Draft template + evidence annex; provide exemplars. Outcome: Final CV template (Word/PDF/Latex) and author guidance.

- **Create scoring rubric & panel handbook** Action: Define dimensions, weights, scoring bands; write bias-mitigation prompts; add calibration examples. Outcome: Panel rubric + handbook.

- **COI, appeals & decision logging** Action: Standardise COI forms, appeals grounds, and a decision register format. Outcome: Form pack + decision log template.

- **Tooling & templates rollout** Action: Update application forms/portals, decision registers, and a lightweight KPI dashboard; add SOPs. Outcome: Live forms, registry, and dashboard v1.

- **Evaluator training & calibration** Action: Deliver micro-modules; run a live calibration on anonymised sample CVs. Outcome: Trained panel pool + calibration report.

- **Pilot on internal calls** Action: Run 1–2 seed/fellowship/award cycles using the new framework. Outcome: Pilot results pack (scores, decisions, feedback, issues).

- **Refine & release v1.1** Action: Address pilot feedback; adjust rubric/metrics/templates. Outcome: Framework v1.1 (change log included).

- **Adoption kit & formal approval** Action: Compile policy text, templates, handbook, training links; table for Alliance approval. Outcome: public documentation set.

- **Institutional rollout support** Action: Provide a turn-key kit for each HEI (comms slides, FAQs) and light coaching. Outcome: freamework introduction to HEI-level, possible adoption plans.

- **Monitoring & annual report** Action: Track KPIs (use of narrative CV, SLA adherence, accuracy/uptake, time-to-decision, EDI); publish a summary. Outcome: **??Annual??** Assessment Report with KPIs and recommendations.

- **Continuous improvement loop** Action: Schedule reviews, audits, and stakeholder consultations; version the framework. Outcome: vNext roadmap + closed actions from audits/post-mortems.


## Glossary

* Cal/Val engagements = “Calibration & Validation” activities for satellite products.
* Micro-credentials = Short, stand-alone certifications for a specific skill
* Train-the-Trainer = A program that prepares selected staff to teach others, thus creating internal multipliers
* Data stewardship = the end-to-end care of data so it’s FAIR (Findable, Accessible, Interoperable, Reusable), secure, well-documented, and preserved—covering the whole lifecycle from planning to long-term archiving.
* OTM-R = Open, Transparent and Merit-based Recruitment (of researchers).
It’s the EU’s standard for fair hiring and career progression in research—part of the Charter & Code / HRS4R framework and fully consistent with CoARA.
* MTTR = Mean Time To Restore/Resolve/Repair
* SLA  = Service Level Agreement — a plain service promise to users
* SOP = Standard Operating Procedure — a step-by-step runbook for routine ops and incidents 
* EURAXESS — the EU’s research jobs portal.
* Bus-factor (# maintainers) — a resilience metric: how many people can leave before a project stalls
* COI = Conflict of Interest

## Related

Issue micro-credentials for POD, Coastal Altimetry, Gravity Processor, GNSS Deformation, plus Open-source & FAIR and RMA/Compliance.

Run TtT cohorts so each widening HEI has at least 2 certified trainers per flagship, who then deliver local sessions and support Living-Lab pilots.
